{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Evolutionary\n",
    "using Flux\n",
    "using Flux: onehot, onecold, logitcrossentropy #, throttle, @epochs\n",
    "using MLDatasets\n",
    "using Random\n",
    "using StableRNGs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using Iris dataset for this exmaple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 150)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = Iris.features();\n",
    "slabels = Iris.labels();\n",
    "classes = unique(slabels)  # unique classes in the dataset\n",
    "nclasses = length(classes) # number of classes\n",
    "d, n = size(features)          # dimension and size if the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert feature and labels in appropriate for training format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150-element Vector{Tuple{SubArray{Float64, 1, Matrix{Float64}, Tuple{Base.Slice{Base.OneTo{Int64}}, Int64}, true}, Flux.OneHotArray{UInt32, 3, 0, 1, UInt32}}}:\n",
       " ([5.1, 3.5, 1.4, 0.2], [1, 0, 0])\n",
       " ([4.9, 3.0, 1.4, 0.2], [1, 0, 0])\n",
       " ([4.7, 3.2, 1.3, 0.2], [1, 0, 0])\n",
       " ([4.6, 3.1, 1.5, 0.2], [1, 0, 0])\n",
       " ([5.0, 3.6, 1.4, 0.2], [1, 0, 0])\n",
       " ([5.4, 3.9, 1.7, 0.4], [1, 0, 0])\n",
       " ([4.6, 3.4, 1.4, 0.3], [1, 0, 0])\n",
       " ([5.0, 3.4, 1.5, 0.2], [1, 0, 0])\n",
       " ([4.4, 2.9, 1.4, 0.2], [1, 0, 0])\n",
       " ([4.9, 3.1, 1.5, 0.1], [1, 0, 0])\n",
       " ([5.4, 3.7, 1.5, 0.2], [1, 0, 0])\n",
       " ([4.8, 3.4, 1.6, 0.2], [1, 0, 0])\n",
       " ([4.8, 3.0, 1.4, 0.1], [1, 0, 0])\n",
       " ⋮\n",
       " ([6.0, 3.0, 4.8, 1.8], [0, 0, 1])\n",
       " ([6.9, 3.1, 5.4, 2.1], [0, 0, 1])\n",
       " ([6.7, 3.1, 5.6, 2.4], [0, 0, 1])\n",
       " ([6.9, 3.1, 5.1, 2.3], [0, 0, 1])\n",
       " ([5.8, 2.7, 5.1, 1.9], [0, 0, 1])\n",
       " ([6.8, 3.2, 5.9, 2.3], [0, 0, 1])\n",
       " ([6.7, 3.3, 5.7, 2.5], [0, 0, 1])\n",
       " ([6.7, 3.0, 5.2, 2.3], [0, 0, 1])\n",
       " ([6.3, 2.5, 5.0, 1.9], [0, 0, 1])\n",
       " ([6.5, 3.0, 5.2, 2.0], [0, 0, 1])\n",
       " ([6.2, 3.4, 5.4, 2.3], [0, 0, 1])\n",
       " ([5.9, 3.0, 5.1, 1.8], [0, 0, 1])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [ (x, onehot(l, classes)) for (x, l) in zip(eachcol(features), slabels)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some auxiliary functions: model accuracy and its loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 3 methods)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model,x,y) = sum(onecold(model(x)) .== onecold(y))/size(x,2)\n",
    "accuracy(xy, model) = sum( onecold(model(x)) .== onecold(y) for (x,y) in xy) /length(xy)\n",
    "\n",
    "loss(model) = (x,y)->logitcrossentropy(model(x), y)\n",
    "loss(model,x,y) = loss(model)(x, y)\n",
    "loss(xy, model) = loss(model)(hcat(map(first,xy)...), hcat(map(last,xy)...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a multi-layer perceptron (MLP) model for our classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(4, 15, relu),                   \u001b[90m# 75 parameters\u001b[39m\n",
       "  Dense(15, 3),                         \u001b[90m# 48 parameters\u001b[39m\n",
       ")\u001b[90m                   # Total: 4 arrays, \u001b[39m123 parameters, 748 bytes."
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Chain(Dense(d, 15, relu), Dense(15, nclasses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model using the backpropagation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(data, model) = 1.1866951880682157\n",
      "accuracy(data, model) = 0.28\n"
     ]
    }
   ],
   "source": [
    "opt = ADAM(1e-4)\n",
    "evalcb = Flux.throttle(() -> @show(loss(data, model), accuracy(data, model)), 5)\n",
    "for i in 1:500\n",
    "    Flux.train!(loss(model), params(model), data, opt, cb = evalcb)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: MLP\n",
      "│   loss = 0.08723255779678181\n",
      "│   accuracy = 0.98\n",
      "└ @ Main In[93]:1\n"
     ]
    }
   ],
   "source": [
    "@info \"MLP\" loss=loss(data, model) accuracy = accuracy(data, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genetic Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we specify a fitness function. We have already defined a loss function for our backpropagation model, so we are going to reuse it.\n",
    "\n",
    "- We pass an individual to the fitness function to evaluate a loss of the MLP.\n",
    "- GA optimization searches an individual to minimize the fitness function. In our case optimization direction is aligned with the backpropagation model loss function as we seek to minimize the MLP loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fitness (generic function with 1 method)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitness(m) = loss(data, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08723255779678181"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitness(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genetic Operators\n",
    "\n",
    "We need to define a crossover operator to combine the information of two parents model to generate new individuals. The objective is to increase genetic variability and provide better options.\n",
    "- Flattent the MLP networks into parameter vector representations\n",
    "- Perform a crossover operation on the parameter vectors\n",
    "- Reconstruct MLPs from the parameter vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uniform_mlp (generic function with 1 method)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function uniform_mlp(m1::T, m2::T; rng::Random.AbstractRNG=Random.default_rng()) where {T <: Chain}\n",
    "    θ1, re1 = Flux.destructure(m1);\n",
    "    θ2, re2 = Flux.destructure(m2);\n",
    "    c1, c2 = UX(θ1,θ2; rng=rng)\n",
    "    return re1(c1), re2(c2)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Chain(Dense(4, 15, relu), Dense(15, 3)), Chain(Dense(4, 15, relu), Dense(15, 3)))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniform_mlp(model, model; rng=StableRNG(42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we rewrite a `gaussian` mutatation operator for MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gaussian_mlp (generic function with 2 methods)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function gaussian_mlp(σ::Real = 1.0)\n",
    "    vop = gaussian(σ)\n",
    "    function mutation(recombinant::T; rng::Random.AbstractRNG=Random.default_rng()) where {T <: Chain}  \n",
    "        θ, re = Flux.destructure(recombinant)\n",
    "        return re(convert(Vector{Float32}, vop(θ; rng=rng)))\n",
    "    end\n",
    "    return mutation\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(4, 15, relu),                   \u001b[90m# 75 parameters\u001b[39m\n",
       "  Dense(15, 3),                         \u001b[90m# 48 parameters\u001b[39m\n",
       ")\u001b[90m                   # Total: 4 arrays, \u001b[39m123 parameters, 748 bytes."
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian_mlp(0.5)(model; rng=StableRNG(42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Population\n",
    "\n",
    "We also require a function for generating a random population of the individuls required for evolutionary optimizations. Our polulation consists of MLP objects, `Flux.Chain` type.\n",
    "\n",
    "We need to override `Evolutionary.initial_population` which will allows us to create population of the random MPL objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initial_population (generic function with 8 methods)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Evolutionary.initial_population\n",
    "function initial_population(method::M, individual::Chain;\n",
    "                            rng::Random.AbstractRNG=Random.default_rng(),\n",
    "                            kwargs...) where {M<:Evolutionary.AbstractOptimizer}\n",
    "    θ, re = Flux.destructure(individual);\n",
    "    [re(randn(rng, length(θ))) for i in 1:Evolutionary.population_size(method)]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Evolutionary` optimization algorithms are designed work with a individual population represented as a collection of numerical vectors.\n",
    "- The optimization objective is kept as a [`EvolutionaryObjective`](https://wildart.github.io/Evolutionary.jl/dev/dev/#Evolutionary.EvolutionaryObjective) object works with any numerical vector type. This object allows to keep a minimizer value, an objective function and its value for minimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvolutionaryObjective{typeof(sum), Float64, Vector{Float64}, Val{:serial}}(sum, 0.0, [NaN, NaN, NaN, NaN, NaN], 0)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EvolutionaryObjective(sum, rand(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this type doesn't have a constructor for working with an MLP object, that is of `Flux.Chain` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvolutionaryObjective{typeof(fitness), Float64, Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, Val{:serial}}(fitness, 0.08723255779678181, Chain(Dense(4, 15, relu), Dense(15, 3)), 0)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EvolutionaryObjective(fitness, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define this missing constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvolutionaryObjective"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Evolutionary.EvolutionaryObjective\n",
    "function EvolutionaryObjective(f, x::Chain; eval::Symbol = :serial)\n",
    "    fval = f(x)\n",
    "    EvolutionaryObjective{typeof(f),typeof(fval),typeof(x),Val{eval}}(f, fval, deepcopy(x), 0)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvolutionaryObjective{typeof(fitness), Float64, Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, Val{:serial}}(fitness, 0.08723255779678181, Chain(Dense(4, 15, relu), Dense(15, 3)), 0)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EvolutionaryObjective(fitness, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionaly, we are required to make a copy of individuals, but `Flux` doesn't provide a `copy` functions for `Chain` objects, only `deepcopy`. We are going to define some missing functions.\n",
    "- Test if your individual object successully makes a copy using `copy` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "copy (generic function with 168 methods)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Base: copy\n",
    "\n",
    "copy(ch::Chain) = deepcopy(ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define the parameters of our evolutionary optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  abstol = Inf\n",
       "                  reltol = Inf\n",
       "        successive_f_tol = 25\n",
       "              iterations = 100\n",
       "             store_trace = false\n",
       "              show_trace = false\n",
       "              show_every = 1\n",
       "                callback = nothing\n",
       "              time_limit = NaN\n",
       "         parallelization = serial\n",
       "                     rng = StableRNGs.LehmerRNG(state=0x00000000000000000000000000000055)\n"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opts = Evolutionary.Options(iterations=100, successive_f_tol=25, rng=StableRNG(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GA[P=150,x=0.9,μ=0.2,ɛ=0.03]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo = GA(\n",
    "        selection = rouletteinv,\n",
    "        mutation =  gaussian_mlp(),\n",
    "        crossover = uniform_mlp,\n",
    "        mutationRate = 0.2,\n",
    "        crossoverRate = 0.9,\n",
    "        populationSize = 150,\n",
    "        ε = 0.03\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " * Status: success\n",
       "\n",
       " * Candidate solution\n",
       "    Minimizer:  Chain(Dense(4, 15, relu), Dense(15, 3))\n",
       "    Minimum:    0.10592845003287427\n",
       "    Iterations: 84\n",
       "\n",
       " * Found with\n",
       "    Algorithm: GA[P=150,x=0.9,μ=0.2,ɛ=0.03]\n",
       "\n",
       " * Convergence measures\n",
       "    |f(x) - f(x')| = 0.0 ≤ 1.0e-12\n",
       "\n",
       " * Work counters\n",
       "    Seconds run:   1.2072 (vs limit Inf)\n",
       "    Iterations:    84\n",
       "    f(x) calls:    12750\n"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = Evolutionary.optimize(fitness, model, algo, opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: MLP\n",
      "│   loss = 0.10592845003287427\n",
      "│   accuracy = 0.98\n",
      "└ @ Main In[109]:2\n"
     ]
    }
   ],
   "source": [
    "evomodel= Evolutionary.minimizer(res)\n",
    "@info \"MLP\" loss=loss(data, evomodel) accuracy = accuracy(data, evomodel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
